"""
Analyze SGD 50-epoch training results to explain:
1. Overfitting pattern (validation-test accuracy gap)
2. SWAG underperformance vs MC Dropout/Ensemble
"""

import json
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

# Load all results
results_dir = Path("c:/Users/lpnhu/Downloads/uq_capstone/results")

methods = {
    'Baseline': 'sgd_50epochs_baseline',
    'MC Dropout': 'sgd_50epochs_mcdropout/mc_dropout_sgd',
    'SWAG': 'sgd_50epochs_swag',
    'Ensemble': 'sgd_50epochs_ensemble/ensemble_sgd/member_1'  # Use first member
}

# Load history for each method
histories = {}
for name, path in methods.items():
    history_path = results_dir / path / "history.json"
    if history_path.exists():
        with open(history_path, 'r') as f:
            h_raw = json.load(f)
            # Normalize structure - handle both formats
            if 'epoch' not in h_raw and 'train_acc' in h_raw:
                # Format like MC Dropout and Ensemble: dict with arrays but no 'epoch' key
                n_epochs = len(h_raw['train_acc'])
                h_raw['epoch'] = list(range(1, n_epochs + 1))
                # Also normalize test_acc vs val_acc
                if 'test_acc' not in h_raw and 'val_acc' in h_raw:
                    h_raw['test_acc'] = h_raw['val_acc']
                    h_raw['test_loss'] = h_raw['val_loss']
            histories[name] = h_raw
        print(f"\n{'='*60}")
        print(f"{name}")
        print(f"{'='*60}")
        
        # Get training progression
        n_epochs = len(h_raw['epoch'])
        
        # Extract key epochs
        epochs_to_show = [0, 9, 24, 49]  # Epochs 1, 10, 25, 50
        print(f"\nTraining progression ({n_epochs} epochs):")
        print(f"{'Epoch':<8} {'Train Acc':<12} {'Val Acc':<12} {'Train Loss':<12} {'Val Loss':<12}")
        print("-" * 60)
        
        for i in epochs_to_show:
            if i < n_epochs:
                train_acc = h_raw['train_acc'][i] if 'train_acc' in h_raw else 0
                test_acc = h_raw['test_acc'][i] if 'test_acc' in h_raw else 0
                train_loss = h_raw['train_loss'][i] if 'train_loss' in h_raw else 0
                test_loss = h_raw['test_loss'][i] if 'test_loss' in h_raw else 0
                print(f"{i+1:<8} {train_acc:>10.2f}%  {test_acc:>10.2f}%  "
                      f"{train_loss:>10.4f}  {test_loss:>10.4f}")
        
        # Find best validation epoch
        test_accs = h_raw.get('test_acc', [])
        best_epoch = np.argmax(test_accs)
        best_test_acc = test_accs[best_epoch]
        
        print(f"\nBest validation: Epoch {best_epoch+1} = {best_test_acc:.2f}%")
        
        # Check if training continued improving after best validation
        train_accs = h_raw.get('train_acc', [])
        final_train_acc = train_accs[-1]
        best_epoch_train_acc = train_accs[best_epoch]
        
        print(f"\nOverfitting analysis:")
        print(f"  Train acc at best val epoch ({best_epoch+1}): {best_epoch_train_acc:.2f}%")
        print(f"  Final train acc (epoch 50): {final_train_acc:.2f}%")
        print(f"  Train improvement after best val: {(final_train_acc - best_epoch_train_acc):.2f}%")
        print(f"  Val degradation after best val: {(test_accs[-1] - best_test_acc):.2f}%")

# Plot training curves
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('SGD 50-Epoch Training Curves - All Methods', fontsize=16, fontweight='bold')

colors = {'Baseline': 'blue', 'MC Dropout': 'green', 'SWAG': 'red', 'Ensemble': 'orange'}

# Plot 1: Training Accuracy
ax = axes[0, 0]
for name, h in histories.items():
    epochs = range(1, len(h['epoch']) + 1)
    train_accs = h.get('train_acc', [])
    ax.plot(epochs, train_accs, label=name, color=colors[name], linewidth=2)
ax.set_xlabel('Epoch', fontsize=12)
ax.set_ylabel('Training Accuracy (%)', fontsize=12)
ax.set_title('Training Accuracy Over Time', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

# Plot 2: Validation Accuracy
ax = axes[0, 1]
for name, h in histories.items():
    epochs = range(1, len(h['epoch']) + 1)
    test_accs = h.get('test_acc', [])
    ax.plot(epochs, test_accs, label=name, color=colors[name], linewidth=2)
    
    # Mark best validation epoch
    best_idx = np.argmax(h.get('test_acc', []))
    best_test = h['test_acc'][best_idx]
    ax.scatter([best_idx + 1], [best_test], color=colors[name], s=100, zorder=5)
    ax.annotate(f'{best_test:.1f}%', xy=(best_idx + 1, best_test), 
                xytext=(5, 5), textcoords='offset points', fontsize=9)

ax.set_xlabel('Epoch', fontsize=12)
ax.set_ylabel('Validation Accuracy (%)', fontsize=12)
ax.set_title('Validation Accuracy Over Time', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

# Plot 3: Train vs Val Gap (Overfitting indicator)
ax = axes[1, 0]
for name, h in histories.items():
    epochs = range(1, len(h['epoch']) + 1)
    train_accs = h.get('train_acc', [])
    test_accs = h.get('test_acc', [])
    gaps = [(train_accs[i] - test_accs[i]) for i in range(len(train_accs))]
    ax.plot(epochs, gaps, label=name, color=colors[name], linewidth=2)

ax.axhline(y=0, color='black', linestyle='--', linewidth=1)
ax.set_xlabel('Epoch', fontsize=12)
ax.set_ylabel('Train - Val Accuracy Gap (%)', fontsize=12)
ax.set_title('Overfitting Gap Over Time', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

# Plot 4: Validation Loss
ax = axes[1, 1]
for name, h in histories.items():
    epochs = range(1, len(h['epoch']) + 1)
    test_losses = h.get('test_loss', [])
    ax.plot(epochs, test_losses, label=name, color=colors[name], linewidth=2)

ax.set_xlabel('Epoch', fontsize=12)
ax.set_ylabel('Validation Loss', fontsize=12)
ax.set_title('Validation Loss Over Time', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_dir / 'sgd_50epochs_training_curves.png', dpi=300, bbox_inches='tight')
print(f"\n{'='*60}")
print(f"Saved training curves to: {results_dir / 'sgd_50epochs_training_curves.png'}")
print(f"{'='*60}")

# SWAG-specific analysis
print(f"\n{'='*60}")
print("SWAG-Specific Analysis")
print(f"{'='*60}")

swag_history = histories.get('SWAG', {})
if swag_history:
    # SWAG collects snapshots from epoch 28-50 (start_epoch=27 in 0-indexed)
    print("\nSWAG Collection Window: Epochs 28-50 (23 snapshots)")
    
    # Check model state at start of SWAG collection
    train_accs = swag_history.get('train_acc', [])
    test_accs = swag_history.get('test_acc', [])
    
    if len(test_accs) >= 27:
        print(f"\nAt start of SWAG collection (Epoch 27):")
        print(f"  Train acc: {train_accs[26]:.2f}%")
        print(f"  Val acc: {test_accs[26]:.2f}%")
        
        # Compare to best validation epoch
        best_epoch = np.argmax(test_accs)
        
        if best_epoch < 27:
            print(f"\n‚ö†Ô∏è  CRITICAL: Best validation occurred at epoch {best_epoch+1}")
            print(f"   This is BEFORE SWAG collection started (epoch 28)!")
            print(f"   Best val acc: {test_accs[best_epoch]:.2f}%")
            print(f"   SWAG collected from overfit region!")
        else:
            print(f"\n‚úì Best validation occurred at epoch {best_epoch+1} (during/after SWAG)")

# Summary comparison
print(f"\n{'='*60}")
print("Final Summary - Why SWAG Underperforms")
print(f"{'='*60}")

print("\nFinal Performance:")
for name in ['Baseline', 'MC Dropout', 'SWAG', 'Ensemble']:
    if name in histories:
        h = histories[name]
        train_accs = h.get('train_acc', [])
        test_accs = h.get('test_acc', [])
        final_train = train_accs[-1] if train_accs else 0
        best_test = max(test_accs) if test_accs else 0
        print(f"  {name:<12}: Train={final_train:>6.2f}%, Best Val={best_test:>6.2f}%")

print("\nüîç Key Findings:")
print("1. All methods achieve ~98-100% training accuracy (severe overfitting)")
print("2. Best validation accuracies diverge significantly:")
print("   - MC Dropout & Ensemble: ~91% (regularization helps)")
print("   - Baseline & SWAG: ~88-90% (no regularization during training)")
print("3. SWAG collects snapshots AFTER model has overfit")
print("   - Small dataset (4,172 samples) causes early overfitting")
print("   - SWAG averaging over overfit models doesn't recover generalization")
print("4. MC Dropout & Ensemble maintain regularization throughout training")
print("   - Dropout prevents overfitting during training")
print("   - Ensemble diversity from random initialization")

plt.show()
