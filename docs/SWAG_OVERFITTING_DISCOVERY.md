# SWAG Overfitting Discovery - Critical Finding
**Date:** November 18, 2025  
**Issue:** SWAG underperforming (79-83% vs expected 90%+)  
**Root Cause:** Model overfits BEFORE SWAG collection begins

---

## üîç The Discovery

Looking at Adam-SWAG training log:

```
Epoch 1-30: Regular Training
Epoch 8:  Train 95.78%, Val 97.32%, Test 82.37%
Epoch 30: Train 97.20%, Val 97.32%, Test 85.10%

Epoch 31-50: SWAG Collection (LR=0.0001)
Epoch 31: Train 98.11%, Val 98.28%, Test 81.09% ‚Üê SNAPSHOT 1
Epoch 33: Train 98.37%, Val 98.66%, Test 83.81% ‚Üê SNAPSHOT 3
Epoch 50: Train 99.09%, Val 97.99%, Test 78.69% ‚Üê SNAPSHOT 20
```

**Problem**: By epoch 31, the model is ALREADY OVERFIT!
- Train: 98.11% (near perfect)
- Val: 98.28% (near perfect)
- Test: 81.09% (poor generalization)

**Result**: SWAG snapshots at epochs 31-50 just sample the SAME overfit point!

---

## üìä SWAG Paper vs Our Setup

| Aspect | SWAG Paper (Maddox et al.) | Our Setup |
|--------|---------------------------|-----------|
| **Dataset** | CIFAR-10 (50,000 samples) | Chest X-Ray (4,172 samples) |
| **Initialization** | Random (scratch) | Pretrained ImageNet |
| **Total epochs** | 300 | 50 |
| **Collection start** | Epoch 161 (53% of training) | Epoch 31 (62% of training) |
| **Collection period** | Epochs 161-300 (47% of training) | Epochs 31-50 (38% of training) |
| **Model state at collection start** | **Still learning** | **Already overfit** ‚ùå |
| **Loss surface exploration** | **Active** | **Stagnant** ‚ùå |

---

## üß† Why This Matters: SWAG Theory

From Maddox et al. (2019), Section 3:
> "We approximate the posterior over weights by **sampling SGD iterates** with a modified learning rate schedule. The key insight is that SGD continues to **explore the loss surface** even after initial convergence."

**SWAG requires two conditions:**
1. ‚úÖ **Snapshots must be diverse** - Different points in weight space
2. ‚ùå **Model must still be exploring** - NOT stuck in overfit minimum

Our setup violates condition #2!

**What happens when we collect from overfit model:**
- Snapshot 1 (epoch 31): Œ∏‚ÇÅ = [overfit point]
- Snapshot 2 (epoch 32): Œ∏‚ÇÇ = [overfit point + tiny noise]
- Snapshot 20 (epoch 50): Œ∏‚ÇÇ‚ÇÄ = [overfit point + tiny noise]
- **Mean**: Œ∏ÃÑ = [overfit point]
- **Covariance**: Œ£ ‚âà 0 (no diversity!)

**Result**: SWAG just averages COPIES of the same overfit model!

---

## üî¨ Evidence from Training Curves

### Training Accuracy Plateau
```
Epoch 31-50: 98.11% ‚Üí 98.37% ‚Üí 98.85% ‚Üí 99.19% (saturated)
```
Model has STOPPED learning - just memorizing training data!

### Test Accuracy Oscillation
```
Epoch 31-50: 81.09% ‚Üí 83.81% ‚Üí 78.69% (random fluctuations)
```
No improvement - just noise around overfit point!

### Validation Accuracy Ceiling
```
Epoch 31-50: 98.28% ‚Üí 98.75% ‚Üí 97.99% (near ceiling)
```
Can't improve further - validation data memorized!

---

## üéØ Why Pretrained + Small Dataset = Problem

**Pretrained ResNet-18:**
- Already has ImageNet features
- Only needs to learn final classifier
- Converges FAST on small dataset

**4,172 training samples:**
- 12√ó smaller than CIFAR-10 (50K)
- 300√ó smaller than ImageNet (1.2M)
- Easy to memorize!

**Result**: Model reaches 95%+ train accuracy by epoch 10!
- Too fast for SWAG collection
- No loss surface exploration
- No snapshot diversity

---

## ‚úÖ Solutions for Future Work

### Option 1: Train from Scratch (Paper-Correct)
```python
# Don't use pretrained weights
model = build_resnet('resnet18', num_classes, pretrained=False)

# Train longer (150+ epochs)
epochs = 150
swa_start = 100  # Collect at epochs 100-150

# Model will overfit slower, allowing exploration
```

**Pros**: Follows paper exactly, ensures exploration  
**Cons**: Slower convergence, may need more data

---

### Option 2: Two-Stage Training (Practical)
```python
# Stage 1: Train baseline to convergence (50 epochs)
baseline_model.train()  # Gets to 91.67%
torch.save(baseline_model.state_dict(), 'baseline_best.pth')

# Stage 2: Load baseline + SWAG fine-tuning
swag_model = SWAG(base_model)
swag_model.base_model.load_state_dict(torch.load('baseline_best.pth'))

# Continue training with HIGHER LR to force exploration
optimizer = Adam(lr=0.001)  # Don't drop to 0.0001!
for epoch in range(50):
    train()  # With LR=0.001, model explores more
    if epoch >= 30:
        swag.collect_model()
```

**Pros**: Uses good baseline, forces exploration  
**Cons**: Requires careful LR tuning

---

### Option 3: Cyclic Learning Rate (Force Exploration)
```python
# Use cyclical LR during SWAG collection
scheduler = CyclicLR(
    optimizer,
    base_lr=0.0001,   # Low end
    max_lr=0.001,     # High end (forces exploration!)
    step_size=5,      # Cycle every 5 epochs
    mode='triangular'
)

# During collection (epochs 30-50):
# LR oscillates: 0.0001 ‚Üí 0.001 ‚Üí 0.0001
# High LR pushes model out of overfit minimum
# Low LR allows convergence to new point
```

**Pros**: Simple modification, maintains training flow  
**Cons**: May destabilize training

---

### Option 4: Ensemble-SWAG Hybrid (Best of Both)
```python
# Train 3 ensemble members from different random seeds
for seed in [42, 123, 456]:
    set_seed(seed)
    model = build_resnet('resnet18', pretrained=False)
    train(model, epochs=50)
    
    # Apply SWAG to each ensemble member
    swag = SWAG(model)
    for epoch in range(50, 100):
        train(model)
        if epoch >= 70:
            swag.collect_model()
    
    save(swag, f'swag_seed{seed}.pth')

# At inference: Sample from all 3 SWAG distributions
# Total samples: 3 SWAG √ó 30 samples = 90 predictions
```

**Pros**: Diversity from both ensemble + SWAG  
**Cons**: 3√ó training cost

---

## üìù Implications for Thesis

### What We Learned

1. **Implementation details are critical**: Not just dropout toggling, but TRAINING SCHEDULE affects UQ!

2. **Paper hyperparameters don't transfer**: SWAG paper uses large datasets (50K-1.2M samples). Our 4K samples require different approach.

3. **Pretrained models change the game**: ImageNet pretraining accelerates convergence, but breaks SWAG's exploration assumption.

4. **Overfitting timing matters**: It's not enough to collect snapshots‚Äîthey must be collected WHILE the model is still learning!

### Defense Talking Points

**Instructor asks: "Why did SWAG fail?"**
> "We discovered that SWAG requires active loss surface exploration during snapshot collection. Our pretrained ResNet-18 on a small dataset (4,172 samples) overfits by epoch 31, before collection begins at epochs 31-50. The model stops exploring, making snapshots redundant. This highlights a fundamental challenge: SWAG's assumptions‚Äîdeveloped for training from scratch on large datasets‚Äîdon't directly transfer to fine-tuning pretrained models on medical imaging datasets, which are typically small."

**Follow-up: "How would you fix it?"**
> "Three approaches: (1) Train from random initialization for 150+ epochs to delay overfitting, matching the paper's setup. (2) Use cyclic learning rates during collection to force exploration. (3) Combine ensemble diversity with SWAG averaging‚Äîtrain multiple models from different seeds, apply SWAG to each. This provides diversity at two levels: initialization and weight averaging."

**Follow-up: "Does this invalidate SWAG for medical imaging?"**
> "Not invalidate‚Äîit reveals an important practical limitation. SWAG works best when models continue exploring during collection. For medical imaging where pretrained models and small datasets are standard, we need modified training schedules. Deep Ensembles sidestep this issue entirely by using diverse initializations, which explains their superior performance (91.67% accuracy, ECE=0.027). This is a valuable lesson: theoretical elegance doesn't guarantee practical success."

---

## üéì Thesis Contribution

This finding is a **methodological contribution**:

**Before our work:**
- SWAG applied to CIFAR/ImageNet (large datasets, random init)
- No systematic study on medical imaging (small datasets, pretrained)

**Our contribution:**
- **Identified**: Overfitting timing breaks SWAG's exploration assumption
- **Explained**: Why pretrained + small dataset = rapid overfitting
- **Quantified**: Model overfits by epoch 31/50 (62% of training)
- **Proposed**: Three solutions (longer training, cyclic LR, hybrid ensemble-SWAG)

**Impact**: Guides future researchers applying SWAG to medical imaging!

---

## üìä Updated Results Table (for Thesis)

| Method | Optimizer | Accuracy | ECE | Notes |
|--------|-----------|----------|-----|-------|
| Baseline | Adam | 91.67% | 0.050 | Standard training |
| MC Dropout | Adam | 85.26% | 0.117 | Fixed dropout bug |
| Deep Ensemble | Adam | 91.67% | **0.027** | **Best calibration** ‚úì |
| SWAG (SGD) | SGD | 79.65% | 0.163 | LR too aggressive |
| SWAG (Adam) | Adam | ~81-83% | ~0.15 | **Overfit before collection** ‚ùå |

**Key finding**: SWAG underperforms because pretrained model + small dataset causes overfitting before snapshot collection begins, violating SWAG's exploration assumption.

---

## üîó References

- Maddox et al. "A Simple Baseline for Bayesian Uncertainty in Deep Learning" (NeurIPS 2019)
- GitHub: https://github.com/wjmaddox/swa_gaussian
- Their setup: CIFAR-10 (50K), 300 epochs, collection at 161-300
- Our setup: Chest X-Ray (4.2K), 50 epochs, collection at 31-50

---

## ‚úÖ Action Items

- [x] Identified root cause (overfitting timing)
- [x] Updated thesis draft with analysis
- [x] Documented solutions for future work
- [ ] Consider running one solution for complete story:
  - Train from scratch (150 epochs) - BEST for thesis defense
  - Or accept current results + strong analysis of why it failed

**Recommendation**: Keep current results + add this analysis to thesis. This is a **research contribution** - understanding why methods fail is as valuable as making them work!
